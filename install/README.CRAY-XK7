Info by Filippo Spiga, Jun. 2013, valid for any version of QE after 5.


0. Tested environments

Machine name    : TODI at CSCS (CH)
Machine spec    : http://user.cscs.ch/hardware/todi_cray_xk7/index.html

Machine name    : TITAN at Oak Ridge National laboratory (USA)
Machine spec    : https://www.olcf.ornl.gov/computing-resources/titan-cray-xk7/

IMPORTANT NOTE: other CRAY XK7 systems might have different modules, please
                check for equivalent if the ones mentioned are missing

 

1. Compile the code

Suggested compiler : PGI


## PGI (usually the default after login) ##

module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load gcc/4.4.4

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling  --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x


## INTEL ##

module switch PrgEnv-pgi PrgEnv-intel
module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load gcc/4.4.4

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling  --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x


## CRAY ##

module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load gcc/4.4.4

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  --disable-wrappers ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  --disable-wrappers ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling --disable-wrappers   --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x


## GNU ##

module switch PrgEnv-pgi PrgEnv-gnu
module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm
module load fftw

export CC="cc"
export FC="ftn"
export CXX="CC"
export MPICC="cc"
export MPIF90="ftn"
export CFLAGS="-O3 -fopenmp"
export FCLAGS="-O3 -fopenmp"
export CXXFLAGS="-O3 -fopenmp"

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --disable-magma --enable-phigemm  --with-scalapack --disable-profiling --with-elpa  ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --enable-magma --enable-phigemm  --without-scalapack --disable-profiling  --with-internal-cblas ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-magma.x



IMPORTANT NOTE : Executables will be located under "./bin"

IMPORTANT NOTE : only pw-gpu.x, neb-gpu.x, ph-gpu.x use extensively 
                 the GPU card in multiple sections of the code. All 
                 the other executable exploit the GPU only by the 
                 phiGEMM library (for now)


    
2. Good practices

- Each NVIDIA Tesla K20 GPU has 6 GB of memory on the card. Better to limit 
  the number of MPI per node (so the number of MPI sharing the same GPU) 
  to 2.

- If the calculation is not too memory demanding, it is possible to increase 
  the ratio MPI:GPU up to 4. The new Hyper-Q technology will help to leverage 
  and exploit the GPU at its best.
  
- In order to share the GPU between multiple MPI processes within the node is 
  mandatory to export the variable CRAY_CUDA_PROXY ("export CRAY_CUDA_PROXY=1")



3.a Example scripts (CSCS, SLURM)

#SBATCH --job-name="QE-BENCH-SPIGA"
#SBATCH --nodes=64
# REMEMBER: --ntasks-per-node * --cpus-per-task <= 16
#SBATCH --ntasks-per-node=2
#SBATCH --cpus-per-task=8
#SBATCH --time=02:00:00
#SBATCH --output=QE-BENCH.%j.o
#SBATCH --error=QE-BENCH.%j.e
#SBATCH --account=<...>

echo "The current job ID is $SLURM_JOB_ID"
echo "Running on $SLURM_NNODES nodes"
echo "Using $SLURM_NTASKS_PER_NODE tasks per node"
echo "A total of $SLURM_NPROCS tasks is used"

export OMP_NUM_THREADS=8
export CRAY_CUDA_PROXY=1

export MALLOC_MMAP_MAX_=0
export MALLOC_TRIM_THRESHOLD_=536870912
#export MPICH_VERSION_DISPLAY=1
#export MPICH_ENV_DISPLAY=1

aprun -n $SLURM_NPROCS -N 2 -d 8 ./pw.x -input <...> | tee out


3.b Example scripts (TITAN, PBS)

IMPORTANT NOTE : The per node charging factor changed from 16 to 30 to 
                 reflect the availability of GPU/Accelerators. Job utilization 
                 is now calculated via the formula: 
                          30 * walltime * number of requested nodes 

IMPORTANT NOTE : project granted at ORNL usually have 3 letters (XXX) 
                 and three digits (YYY)

#!/bin/csh
#PBS -A XXXYYY
#PBS -N QE-BENCH-SPIGA
#PBS -j oe
#PBS -l walltime=2:00:00,nodes=8

cd $PBS_O_WORKDIR

setenv OMP_NUM_THREADS 4
setenv MKL_NUM_THREADS 4

setenv MPICH_ENV_DISPLAY 1
setenv PMI_DEBUG 1

# REMEMBER TO CALCULATE...
# '-n' : number of PEs or total MPI processes ('-N' \times 'nodes') 
# '-N' : number MPI task per node
# '-d' : number OpenMP thread per node
# so...
# nodes \times 16 = total number of cores requested
# '-d' \times '-N' == 16
 
aprun -n 32 -N 4 -d 4 ./pw-gpu.x -input <...> | tee out.$PBS_JOBID


IMPORTANT NOTE : refer to this link 
                    https://www.olcf.ornl.gov/kb_articles/spider-the-center-wide-lustre-file-system/
                 about the fileystem management and availability


4. Benchmarks 
